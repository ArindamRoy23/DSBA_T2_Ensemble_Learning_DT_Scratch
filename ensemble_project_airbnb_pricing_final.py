# -*- coding: utf-8 -*-
"""Ensemble Project - Airbnb pricing- Final Copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uu8BeUmhPu_YwaPJt5ymtzSW5I-toXIr
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import urllib
import io

from scipy.stats import chi2_contingency

from sklearn.preprocessing import QuantileTransformer
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
!pip install xgboost
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score, make_scorer

data = pd.read_csv('/content/AB_NYC_2019.csv')

data.head()

data.describe()

"""#descriptive analysis"""

print(pd.to_datetime(data['last_review']).min())
print(pd.to_datetime(data['last_review']).max())

#transform datetype
data['last_review'] = pd.to_datetime(data['last_review'])

data.shape

data.isnull().sum()

#fill the null value
reference_date = pd.to_datetime('2019-12-31')

# create a new column "recency_of_review" that contains the difference in days between "last_review" and reference_date
data['recency_of_review'] = (reference_date - data['last_review']).dt.days

# replace null values in "recency_of_review" with random values between 3500 and 4000
data['recency_of_review'].fillna(value=np.random.randint(3500, 3700), inplace=True)

data['reviews_per_month'].describe()

#fill the null value with median
data['reviews_per_month'].fillna(value=0.7, inplace=True)

data = data.dropna()

#drop the unrelevant features
data = data.drop(columns = ['id','host_id','name','host_name','last_review'])

data.isnull().sum()

data['neighbourhood_group'].nunique()

data['neighbourhood'].nunique()

data['neighbourhood_group'].value_counts()

#Heatmap for Quantitative
quanti=data[['price','longitude','latitude','minimum_nights','number_of_reviews','reviews_per_month','calculated_host_listings_count','availability_365','recency_of_review']]
sns.heatmap(quanti.corr(),annot=True,cmap='GnBu',linewidths=0.2)
fig=plt.gcf()
fig.set_size_inches(10,8)
plt.show()

#Alternative variable = reviews_per_month x number_of_reviews ?

#we can see from our statistical table that we have some extreme values, therefore we need to remove them for the sake of a better visualization

#creating a sub-dataframe with no extreme values / less than 500
sub_data=data[data.price < 500]
#using violinplot to showcase density and distribtuion of prices 
viz_neighbourhoodprices=sns.boxplot(data=sub_data, x='neighbourhood_group', y='price')
viz_neighbourhoodprices.set_title('Density and distribution of prices for each neighberhood_group')

#finding out top 10 neighbourhoods
data.neighbourhood.value_counts().head(10)

#let's now combine this with our boroughs and room type for a rich visualization we can make

#grabbing top 10 neighbourhoods for sub-dataframe
sub_topneighbourhood=data.loc[data['neighbourhood'].isin(['Williamsburg','Bedford-Stuyvesant','Harlem','Bushwick',
                 'Upper West Side','Hell\'s Kitchen','East Village','Upper East Side','Crown Heights','Midtown'])]
#using catplot to represent multiple interesting attributes together and a count
viz_topneighbourhood=sns.catplot(x='neighbourhood', hue='neighbourhood_group', col='room_type', data=sub_topneighbourhood, kind='count')
viz_topneighbourhood.set_xticklabels(rotation=90)

#let's what we can do with our given longtitude and latitude columns

#let's see how scatterplot will come out 
viz_longlat=sub_data.plot(kind='scatter', x='longitude', y='latitude', label='availability_365', c='price',
                  cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.4, figsize=(10,8))
viz_longlat.legend()

#initializing the figure size
plt.figure(figsize=(10,8))

#loading the png NYC image found on Google and saving to my local folder along with the project
i=urllib.request.urlopen('https://upload.wikimedia.org/wikipedia/commons/e/ec/Neighbourhoods_New_York_City_Map.PNG').read()
nyc_img=plt.imread(io.BytesIO(i))

#scaling the image based on the latitude and longitude max and mins for proper output
plt.imshow(nyc_img,zorder=0,extent=[-74.258, -73.7, 40.49,40.92])
ax=plt.gca()

#using scatterplot again
sub_data.plot(kind='scatter', x='longitude', y='latitude', label='availability_365', c='price', ax=ax, 
           cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.4, zorder=5)

plt.legend()
plt.show()

#the number of neighbourhood_group
sns.countplot(data['neighbourhood_group'], palette="plasma")
fig = plt.gcf()
fig.set_size_inches(10,10)
plt.title('Neighbourhood Group')

#the availability by neighbourhood_group
plt.figure(figsize=(10,10))
ax = sns.boxplot(data=data, x='neighbourhood_group',y='availability_365',palette='plasma')

#the geo distribution of neighborhood_group
plt.figure(figsize=(10,6))
sns.scatterplot(data.longitude,data.latitude,hue=data.neighbourhood_group)
plt.ioff()

#the density of price
plt.figure(figsize=(30, 30))
sns.displot(sub_data, x="price", kind="kde")

#density of price by room_type
plt.figure(figsize=(30, 30))
sns.displot(sub_data, x="price", hue="room_type", kind="kde")

#density of price by neighbourhood_group
plt.figure(figsize=(30, 30))
sns.displot(sub_data, x="price", hue="neighbourhood_group", kind="kde")

sns.boxplot(data=sub_data, y="price",x="room_type")

sns.countplot(x="room_type", data=sub_data)
plt.title('Number of listings per room type')
plt.xlabel('room type')
plt.ylabel('Flats')

sns.histplot(data['availability_365'], kde=True)

# add labels and title
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Listings per availability')

# show the plot
plt.show()

sns.histplot(data['calculated_host_listings_count'], kde=True)

# add labels and title
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Flats per host listings')
plt.ylim(0,300)
# show the plot
plt.show()

sns.countplot(x="calculated_host_listings_count", data=sub_data)
plt.title('Flats per host listings')
plt.xlabel('Value')
plt.ylabel('Count')

sns.histplot(data['minimum_nights'], kde=True)

# add labels and title
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Flats per min nights')
plt.ylim(0,1000)
plt.xlim(0,100)
# show the plot
plt.show()

sns.countplot(x="minimum_nights", data=sub_data)
plt.title('Number of listings per min number of nights')
plt.xlabel('Min number of nights')
plt.ylabel('Flats')

"""#preprocessing"""

#make a copy
df = data

df=df.drop('neighbourhood', axis=1)

prefixes = {'neighbourhood_group': 'area', 'room_type': 'category'}

# perform one hot encoding
one_hot_encoded = pd.get_dummies(df, prefix=prefixes, columns=['neighbourhood_group', 'room_type'])

# concatenate the encoded data with the original dataframe
df = pd.concat([one_hot_encoded], axis=1)

print(df)

df = df.dropna()

# check for missing values in the data
print(df.isnull().sum())

# create contingency table
contingency_table = pd.crosstab(df['price'], df['availability_365'])

# perform chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# print results
print('Chi-square statistic:', chi2_stat)
print('P-value:', p_val)
print('Degrees of freedom:', dof)
print('Expected frequencies:', expected)

# create contingency table
contingency_table = pd.crosstab(df['price'], df['calculated_host_listings_count'])

# perform chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# print results
print('Chi-square statistic:', chi2_stat)
print('P-value:', p_val)
print('Degrees of freedom:', dof)
print('Expected frequencies:', expected)

# create contingency table
contingency_table = pd.crosstab(df['price'], df['minimum_nights'])

# perform chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# print results
print('Chi-square statistic:', chi2_stat)
print('P-value:', p_val)
print('Degrees of freedom:', dof)
print('Expected frequencies:', expected)

sns.histplot(data['availability_365'], kde=True)

# add labels and title
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Listings per availability')

# show the plot
plt.show()

sns.histplot(data['calculated_host_listings_count'], kde=True)

# add labels and title
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Flats per host listings')
plt.ylim(0,300)
# show the plot
plt.show()

sns.histplot(data['minimum_nights'], kde=True)

# add labels and title
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Flats per min nights')
plt.ylim(0,1000)
plt.xlim(0,100)
# show the plot
plt.show()

# most values of the above features are concentrated in small scale, so we add three new features 

def condition_short_stay(x):
    if x < 2:
        return 1
    else:
        return 0

df['Short_stay'] = df['minimum_nights'].apply(condition_short_stay)

def condition_unfrequent_host(x):
    if x < 10:
        return 1
    else:
        return 0

df['Unfrequent_host'] = df['calculated_host_listings_count'].apply(condition_unfrequent_host)

def condition_generally_available(x):
    if x > 30:
        return 1
    else:
        return 0

df['Availability'] = df['availability_365'].apply(condition_generally_available)

#normalize the numerical feature
col = ['minimum_nights','number_of_reviews','reviews_per_month','calculated_host_listings_count','recency_of_review','availability_365']
qt = QuantileTransformer(output_distribution='normal')
df[col] = qt.fit_transform(df[col])
#take log of the target feature
df['price'] = np.log(df['price']+1)

"""#model"""

# split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('price', axis=1), df['price'], test_size=0.3, random_state=42)

#decision tree
dt = DecisionTreeRegressor(max_depth = 8,min_samples_leaf =4, min_samples_split = 2)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

# Calculate the metrics
mae = mean_absolute_error(np.exp(y_test), np.exp(y_pred))
rmse = mean_squared_error(np.exp(y_test), np.exp(y_pred),squared=False)
r2 = r2_score(y_test, y_pred)
print(f'Mean Absolute Error: {mae}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R2 Score: {r2}')

param_grid = {
    'max_depth': [8, 10, 12],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2, 4],
}

# Create a Random Forest Regressor
dt_model = DecisionTreeRegressor()
'''
scoring = {'mse': make_scorer(mean_squared_error),
           'r2': make_scorer(r2_score),
           'mae':make_scorer(mean_absolute_error)}
'''
scoring = make_scorer(mean_squared_error, squared=False)
# Use Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and R-squared score
print("Best hyperparameters: ", grid_search.best_params_)
print("Best score:", np.sqrt(grid_search.best_score_))

# initialize the random forest regressor
rf = RandomForestRegressor(n_estimators=250, max_depth=15, min_samples_leaf=2, min_samples_split=2, random_state=42)

# fit the model to the training data
rf.fit(X_train, y_train)

# make predictions on the test data
y_pred = rf.predict(X_test)

# calculate the mean squared error and R^2 score of the model
mae = mean_absolute_error(np.exp(y_test), np.exp(y_pred))
rmse = mean_squared_error(np.exp(y_test), np.exp(y_pred),squared=False)
r2 = r2_score(y_test, y_pred)
print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {rmse}')
print(f'R2 Score: {r2}')

param_grid = {
    'n_estimators':[200,250],
    'max_depth': [12,15],
    'min_samples_split': [2],
    'min_samples_leaf': [2],
}

# Create a Random Forest Regressor
rf_model = RandomForestRegressor()

# Use Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='r2')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and R-squared score
print("Best hyperparameters: ", grid_search.best_params_)
print("Best score:", np.sqrt(grid_search.best_score_))

#adaboost
ada = AdaBoostRegressor(n_estimators=60, learning_rate=0.04, random_state=42)

# Fit the AdaBoost regressor to the training data
ada.fit(X_train, y_train)

# Evaluate the performance of the AdaBoost regressor on the testing data
y_pred = ada.predict(X_test)
mae = mean_absolute_error(np.exp(y_test), np.exp(y_pred))
rmse = mean_squared_error(np.exp(y_test), np.exp(y_pred),squared=False)
r2 = r2_score(y_test, y_pred)
print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {rmse}')
print(f'R2 Score: {r2}')

param_grid = {
    'n_estimators':[60,80,100],
    'learning_rate':[0.04,0.05]
}

# Create a Random Forest Regressor
ada_model = AdaBoostRegressor()

# Use Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator=ada_model, param_grid=param_grid, cv=3, scoring='r2')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and R-squared score
print("Best hyperparameters: ", grid_search.best_params_)
print("Best score:", np.sqrt(grid_search.best_score_))

gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05, max_depth=8, random_state=42)

# Train the model
gb_model.fit(X_train, y_train)
y_pred = gb_model.predict(X_test)

mae = mean_absolute_error(np.exp(y_test), np.exp(y_pred))
rmse = mean_squared_error(np.exp(y_test), np.exp(y_pred),squared=False)
r2 = r2_score(y_test, y_pred)
print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {rmse}')
print(f'R2 Score: {r2}')

param_grid = {
    'n_estimators':[100,120,150],
    'max_depth': [8,10,12],
}

# Create a Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()

# Use Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='r2')
grid_search.fit(X_train, y_train)


# Print the best hyperparameters and R-squared score
print("Best hyperparameters: ", grid_search.best_params_)
print("Best score:", np.sqrt(grid_search.best_score_))

import xgboost as xgb
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=150, max_depth=10, learning_rate=0.05, random_state=42)

# Train the model on the training set
xgb_model.fit(X_train, y_train)

# Make predictions on the training and testing sets

y_pred = xgb_model.predict(X_test)

mae = mean_absolute_error(np.exp(y_test), np.exp(y_pred))
rmse = mean_squared_error(np.exp(y_test), np.exp(y_pred),squared=False)
r2 = r2_score(y_test, y_pred)
print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {rmse}')
print(f'R2 Score: {r2}')

param_grid = {
    'n_estimators':[150,180,200],
    'max_depth': [10,12,15],
    'learning_rate':[0.05]
}

# Create a Random Forest Regressor
xgb_model = xgb.XGBRegressor()

# Use Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='r2')
grid_search.fit(X_train, y_train)


# Print the best hyperparameters and R-squared score
print("Best hyperparameters: ", grid_search.best_params_)
print("Best score:", np.sqrt(grid_search.best_score_))